{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad5ffd7-49ca-4f6e-8b9c-77632129bdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adhocmaster\\anaconda3\\envs\\diffusion-road\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6c814c2-68b9-49d7-9c87-47506d523790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d49377-d023-4086-8d97-5d7618fc5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "randImg = (np.random.sample((64, 64)) * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6a6a8f7-7e59-4116-bdfe-f1b86430d87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc0eba9-a96c-4388-aff4-6969ba52f53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40, 254, 111, ..., 157, 121, 214],\n",
       "       [227,   1, 157, ...,  75,   1,   6],\n",
       "       [ 20, 198, 123, ..., 205,  66, 232],\n",
       "       ...,\n",
       "       [ 11,  61, 193, ..., 188, 162, 164],\n",
       "       [169,  23,  33, ...,  70, 246, 118],\n",
       "       [ 13,  15, 168, ..., 103, 147, 107]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7c91ee-86e7-4af0-aa37-26a31e197a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.fromarray(randImg)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9321787b-b753-48c5-b578-45cf4fff950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imT = torch.from_numpy(np.array(randImg)).unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6763fdff-669d-4da1-bf01-164301a09e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c08f0017-f1fe-42c3-b4a0-c1413ce6cfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 40, 254, 111,  ..., 157, 121, 214],\n",
       "          [227,   1, 157,  ...,  75,   1,   6],\n",
       "          [ 20, 198, 123,  ..., 205,  66, 232],\n",
       "          ...,\n",
       "          [ 11,  61, 193,  ..., 188, 162, 164],\n",
       "          [169,  23,  33,  ...,  70, 246, 118],\n",
       "          [ 13,  15, 168,  ..., 103, 147, 107]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dee3d03-a51a-4d3e-8ba7-7ba364711098",
   "metadata": {},
   "outputs": [],
   "source": [
    "toNp = np.array(imT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acf17064-1e57-497b-b25e-6d515be54435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 40, 254, 111, ..., 157, 121, 214],\n",
       "         [227,   1, 157, ...,  75,   1,   6],\n",
       "         [ 20, 198, 123, ..., 205,  66, 232],\n",
       "         ...,\n",
       "         [ 11,  61, 193, ..., 188, 162, 164],\n",
       "         [169,  23,  33, ...,  70, 246, 118],\n",
       "         [ 13,  15, 168, ..., 103, 147, 107]]]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toNp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34ad3ab3-c817-420c-b74e-7b01633b6d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.fromarray(toNp.squeeze())\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20c1fd7d-963c-44c1-bd0c-c5ee1edbd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np2 = np.expand_dims(np.expand_dims(randImg, axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feb59c61-752c-4f92-9cb5-a19422cf2f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 64, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7a78b7-e0c2-4aa7-955c-a27dac4b7a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.fromarray(np2.squeeze())\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691cf40a-57b1-426d-9c3d-763573b38de1",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7762156-f725-40a1-846f-769cfdf2f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from random import randint\n",
    "\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "root.addHandler(handler)\n",
    "\n",
    "from mapdataset import ImageGroupReader, single_layer_converter, MapsDataset, MapReader, ImageUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abea5e96-9740-4efa-9fad-0d9a0e1cb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dillFolder = \"../data/output/SF_Layered/32x32/group-1280-stride-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df3e64dd-29f1-481d-8bf4-ba09a3430a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-04 13:49:28,546 - root - INFO - Loading 6400 patches from ../data/output/SF_Layered/32x32/group-1280-stride-10\n",
      "2022-11-04 13:49:28,547 - root - INFO - reading patch 3599\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAApElEQVR4nJ2SQRLDIAwDVx7+/2X10ECwA2RanwAvsgyWEZgeqssAG7GNBoA1REQXtNwBy0PbuQ4xFKYy19o3UIkpIl967oIcD524MwsXQOiFCKucKROxc9/Pq8lHzICRqg1F6sx4emUApxIWmv7Bukqk9pwvFJN1MqzXLtTOef/2Dv8D8gFop5EHEN7uviVO8t3DLbCgcxcLov7FEVj206bkcjo/SJM9Qto5UuMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapsDataset = MapsDataset(\n",
    "    patch_size=(32, 32), \n",
    "    stride=10, \n",
    "    sample_group_size=1280, \n",
    "    converter=single_layer_converter,\n",
    "    outputDir=\"./data/output\"\n",
    "    ) \n",
    "\n",
    "mapsDataset.loadPatches(dillFolder)\n",
    "patchNo = randint(0, len(mapsDataset))\n",
    "logging.info(f\"reading patch {patchNo}\")\n",
    "patch = mapsDataset[patchNo]\n",
    "\n",
    "# im = ImageUtils.PILPatchToPILImg(patch)\n",
    "# im = ImageUtils.PILPatchToPILImg(patch)\n",
    "im = ImageUtils.TorchNpPatchToPILImgGray(patch)\n",
    "# path = os.path.join(dillFolder, f\"{patchNo}.png\")\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7683d505-1615-4f81-8dfb-4d057b1b0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gather(consts: torch.Tensor, t: torch.Tensor):\n",
    "    \"\"\"Gather consts for $t$ and reshape to feature map shape\"\"\"\n",
    "    c = consts.gather(-1, t)\n",
    "    return c.reshape(-1, 1, 1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8202cbf-494a-4b01-97f6-d4c62f333132",
   "metadata": {},
   "source": [
    "## 2.2 Adding Noise\n",
    "\n",
    "First, we want to define the spet that adds a little bit of noise - $q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1})$:\n",
    "\n",
    "![process](https://datasciencecastnethome.files.wordpress.com/2022/04/screenshot-from-2022-04-12-14-55-09.png) [(Image source)](https://hojonathanho.github.io/diffusion/)\n",
    "\n",
    "We set up a 'variance schedule' β, where beta[t] ($\\beta_t$)specifies how much noise we want to add at that step. You get fancy schedules but we'll stick with a linear one for now. The formula you'll see for this single noise step is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22ce3b7-c7b3-44ff-af4f-5e5e2db73862",
   "metadata": {},
   "source": [
    "# Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e54f9d89-dae1-485e-b6fc-289f38cbe840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adhocmaster\\anaconda3\\envs\\diffusion-road\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=640x128>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = 100\n",
    "beta = torch.linspace(0.0001, 0.04, n_steps)\n",
    "inputSide = 32\n",
    "\n",
    "def q_xt_xtminus1(xtm1, t):\n",
    "  mean = gather(1. - beta, t) ** 0.5 * xtm1 # √(1−βt)*xtm1\n",
    "  var = gather(beta, t) # βt I\n",
    "  eps = torch.randn_like(xtm1) # Noise shaped like xtm1\n",
    "  return mean + (var ** 0.5) * eps\n",
    "\n",
    "# Show im at different stages\n",
    "ims = []\n",
    "start_im = ImageUtils.TorchNpPatchToPILImgGray(mapsDataset[0])\n",
    "x = mapsDataset[0]\n",
    "# print(x.shape)\n",
    "for t in range(n_steps):\n",
    "\n",
    "  # Store images every 20 steps to show progression\n",
    "  if t%20 == 0:\n",
    "    ims.append(ImageUtils.TorchNpPatchToPILImgGray(x))\n",
    "  \n",
    "  # Calculate Xt given Xt-1 (i.e. x from the previous iteration)\n",
    "  t = torch.tensor(t, dtype=torch.long) # t as a tensor\n",
    "  x = q_xt_xtminus1(x, t) # Modify x using our function above\n",
    "  # print(x.shape)\n",
    "\n",
    "# Display the images\n",
    "image = Image.new('L', size=(inputSide*5, inputSide))\n",
    "for i, im in enumerate(ims):\n",
    "  image.paste(im, ((i%5)*inputSide, 0))\n",
    "image.resize((inputSide*4*5, inputSide*4), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafcdcc0-50a6-475e-9714-d3317050fc4b",
   "metadata": {},
   "source": [
    "Now, we want to train our model at different time steps and we don't particulary want to iterativly add little bits of noise a bunch of times just to train one sample from t=37. \n",
    "\n",
    "Luckily, some smart people did some fancy maths (link https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick) using something called the reparameterization trick that lets us get $x_t$ for any t given $x_0$. \n",
    "\n",
    "$\\begin{aligned}\n",
    "q(\\mathbf{x}_t \\vert \\mathbf{x}_0) &= \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})\n",
    "\\end{aligned}$ where $\\bar{\\alpha}_t = \\prod_{i=1}^T \\alpha_i$\n",
    "\n",
    "Again, the code is far less scary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "897800ca-495e-4577-a41e-25abd162aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Unet Definition\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# A fancy activation function\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Swish actiavation function\n",
    "    $$x \\cdot \\sigma(x)$$\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "# The time embedding \n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Embeddings for $t$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        # Activation\n",
    "        self.act = Swish()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        # Create sinusoidal position embeddings\n",
    "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
    "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
    "        # \\end{align}\n",
    "        #\n",
    "        # where $d$ is `half_dim`\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "\n",
    "        #\n",
    "        return emb\n",
    "\n",
    "# Residual blocks include 'skip' connections\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Residual block\n",
    "    A residual block has two convolution layers with group normalization.\n",
    "    Each resolution is processed with two residual blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `in_channels` is the number of input channels\n",
    "        * `out_channels` is the number of input channels\n",
    "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Group normalization and the first convolution layer\n",
    "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "        self.act1 = Swish()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Group normalization and the second convolution layer\n",
    "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        self.act2 = Swish()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Linear layer for time embeddings\n",
    "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        # Add time embeddings\n",
    "        h += self.time_emb(t)[:, :, None, None]\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.act2(self.norm2(h)))\n",
    "\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "# Ahh yes, magical attention...\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Attention block\n",
    "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of channels in the input\n",
    "        * `n_heads` is the number of heads in multi-head attention\n",
    "        * `d_k` is the number of dimensions in each head\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k ** -0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=1)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "\n",
    "        #\n",
    "        return res\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Down block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Up block\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Middle block\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, time_channels: int):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "        self.attn = AttentionBlock(n_channels)\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res1(x, t)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, t)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale up the feature map by $2 \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "# The core class definition (aka the important bit)\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ## U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_channels: int = 3, n_channels: int = 64,\n",
    "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "                 is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "                 n_blocks: int = 2):\n",
    "        \"\"\"\n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "        # Time embedding layer. Time embedding has `n_channels * 4` channels\n",
    "        self.time_emb = TimeEmbedding(n_channels * 4)\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        # Final normalization and convolution layer\n",
    "        self.norm = nn.GroupNorm(8, n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, t)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x, t)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02cccdf9-4a7b-4d0a-bebe-9aca93338178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 32, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see it in action on dummy data:\n",
    "\n",
    "# A dummy batch of 10 1-channel 32px images\n",
    "x = torch.randn(10, 1, inputSide, inputSide)\n",
    "\n",
    "# 't' - what timestep are we on\n",
    "t = torch.tensor([50.], dtype=torch.long)\n",
    "\n",
    "# Define the unet model\n",
    "unet = UNet(image_channels=1)\n",
    "\n",
    "# The foreward pass (takes both x and t)\n",
    "model_output = unet(x, t)\n",
    "\n",
    "# The output shape matches the input.\n",
    "model_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb5d64-8a32-4828-999e-c95be6a9dd33",
   "metadata": {},
   "source": [
    "## 2.4 Training Time\n",
    "\n",
    "Now that we have our 'diffusion model' defined, we need to train it to predict the noise given $x_t$ and $t$.\n",
    "\n",
    "Why not predict the denoised image directly? Mostly just due to convenience - the noise is nicely scaled with a mean of zero, and this well-suited to being modeled with a neural network. You don't **have** to do it this way, but the papers do, and we can make it easy by tweaking our `q_xt_x0` function to return both the noised image ($x_t$) and the noise itself, which will be the 'target' our network tries to produce.\n",
    "\n",
    "This training loop should look familiar from all the past lessons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd0f5a2d-8821-42fd-b512-a22d7e9abd10",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16988\\2082192259.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputSide\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Set up some parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\diffusion-road\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \"\"\"\n\u001b[1;32m--> 689\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\diffusion-road\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\diffusion-road\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\diffusion-road\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \"\"\"\n\u001b[1;32m--> 689\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\diffusion-road\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "unet = UNet(image_channels=1, n_channels=inputSide).cuda()\n",
    "\n",
    "# Set up some parameters\n",
    "n_steps = 100\n",
    "beta = torch.linspace(0.0001, 0.04, n_steps).cuda()\n",
    "alpha = 1. - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "# Modified to return the noise itself as well\n",
    "def q_xt_x0(x0, t):\n",
    "  mean = gather(alpha_bar, t) ** 0.5 * x0\n",
    "  var = 1-gather(alpha_bar, t)\n",
    "  eps = torch.randn_like(x0).to(x0.device)\n",
    "  return mean + (var ** 0.5) * eps, eps # also returns noise\n",
    "\n",
    "# Training params\n",
    "batch_size = 64 # Lower this if hitting memory issues\n",
    "lr = 2e-4 # Explore this - might want it lower when training on the full dataset\n",
    "\n",
    "losses = [] # Store losses for later plotting\n",
    "\n",
    "dataset = mapsDataset#.select(range(10000)) # to use a 10k subset for demo\n",
    "\n",
    "optim = torch.optim.AdamW(unet.parameters(), lr=lr) # Optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e382d-11a2-4cfd-b8f1-833db26dbd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for e in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "  for i in tqdm(range(0, len(dataset)-batch_size, batch_size), position=1, leave=False): # Run through the dataset\n",
    "    ims_tensor = [dataset[idx] for idx in range(i,i+batch_size)] # Fetch some images\n",
    "    tims = [im.cuda() for im in ims_tensor] # Convert to tensors\n",
    "    x0 = torch.cat(tims) # Combine into a batch\n",
    "\n",
    "    t = torch.randint(0, n_steps, (batch_size,), dtype=torch.long).cuda() # Random 't's \n",
    "    xt, noise = q_xt_x0(x0, t) # Get the noised images (xt) and the noise (our target)\n",
    "\n",
    "    pred_noise = unet(xt.float(), t) # Run xt through the network to get its predictions\n",
    "\n",
    "    loss = F.mse_loss(noise.float(), pred_noise) # Compare the predictions with the targets\n",
    "    losses.append(loss.item()) # Store the loss for later viewing\n",
    "\n",
    "    optim.zero_grad() # Zero the gradients\n",
    "    loss.backward() # Backpropagate the loss (computes and store gradients)\n",
    "    optim.step() # Update the network parameters (using those gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111aab4-a23f-4796-8a04-d691155d0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)\n",
    "torch.save(unet, \"diffusion-road64x64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bc9a2-15ce-49df-a2cd-ef2655ad6f1f",
   "metadata": {},
   "source": [
    "## 2.5 The Reverse Step\n",
    "Now we need to define the reverse step $p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)$\n",
    "\n",
    "See that little $_\\theta$? That often indicates 'learned parameters' - in this case our unet model! We use our model to predict the noise and then 'undo' the forward noise steps one at a time to go from an image that is pure noise to one that (hopefully) looks like a real image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f322317c-5ab0-4bd3-83ee-c1607b3d2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_xt(xt, noise, t):\n",
    "  alpha_t = gather(alpha, t)\n",
    "  alpha_bar_t = gather(alpha_bar, t)\n",
    "  eps_coef = (1 - alpha_t) / (1 - alpha_bar_t) ** .5\n",
    "  mean = 1 / (alpha_t ** 0.5) * (xt - eps_coef * noise) # Note minus sign\n",
    "  var = gather(beta, t)\n",
    "  eps = torch.randn(xt.shape, device=xt.device)\n",
    "  return mean + (var ** 0.5) * eps \n",
    "\n",
    "x = torch.randn(1, 1, inputSide, inputSide).cuda() # Start with random noise\n",
    "ims = []\n",
    "for i in range(n_steps):\n",
    "  t = torch.tensor(n_steps-i-1, dtype=torch.long).cuda()\n",
    "  with torch.no_grad():\n",
    "    pred_noise = unet(x.float(), t.unsqueeze(0))\n",
    "    x = p_xt(x, pred_noise, t.unsqueeze(0))\n",
    "    if i%24 == 0:\n",
    "      ims.append(tensor_to_image_gray(x.cpu()))\n",
    "\n",
    "image = Image.new('L', size=(inputSide*5, inputSide))\n",
    "for i, im in enumerate(ims[:5]):\n",
    "  image.paste(im, ((i%5)*inputSide, 0))\n",
    "image.resize((inputSide*4*5, inputSide*4), Image.NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabd94a-c646-403e-bef1-d9fd7172c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Make and show 10 examples:\n",
    "x = torch.randn(10, 1, inputSide, inputSide).cuda() # Start with random noise\n",
    "ims = []\n",
    "for i in range(n_steps):\n",
    "  t = torch.tensor(n_steps-i-1, dtype=torch.long).cuda()\n",
    "  with torch.no_grad():\n",
    "    pred_noise = unet(x.float(), t.unsqueeze(0))\n",
    "    x = p_xt(x, pred_noise, t.unsqueeze(0))\n",
    "\n",
    "for i in range(10):\n",
    "  ims.append(tensor_to_image_gray(x[i].unsqueeze(0).cpu()))\n",
    "\n",
    "image = Image.new('L', size=(inputSide*5, inputSide*2))\n",
    "for i, im in enumerate(ims):\n",
    "  image.paste(im, ((i%5)*inputSide, inputSide*(i//5)))\n",
    "image.resize((inputSide*4*5, inputSide*4*2), Image.NEAREST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
